{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID                                        Description  \\\n",
      "0  id10326  The room was kind of clean but had a VERY stro...   \n",
      "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
      "2  id10328  I booked this hotel through Hotwire at the low...   \n",
      "\n",
      "        Browser_Used Device_Used Is_Response  \n",
      "0               Edge      Mobile   not happy  \n",
      "1  Internet Explorer      Mobile   not happy  \n",
      "2            Mozilla      Tablet   not happy  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#df = pd.Dataframe()\n",
    "df_train = pd.read_csv('./measuring-customer-happiness/train_hp.csv', encoding='utf-8')\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "X = []\n",
    "sentences = list(df_train['Description'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stayed here with husband and sons on the way to an Alaska Cruise We all loved the hotel great experience Ask for room on the North tower facing north west for the best views We had high floor with stunning view of the needle the city and even the cruise ships We ordered room service for dinner so we could enjoy the perfect views Room service dinners were delicious too You are in perfect spot to walk everywhere so enjoy the city Almost forgot Heavenly beds were heavenly too '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification for happy and not_happy \n",
    "\n",
    "y = df_train['Is_Response']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"happy\" else 0, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding layer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('./glove.twitter.27B/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          4154400   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 4,164,401\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 4,154,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/6\n",
      "24916/24916 [==============================] - 1s 40us/step - loss: 0.4898 - acc: 0.7654 - val_loss: 0.4423 - val_acc: 0.7952\n",
      "Epoch 2/6\n",
      "24916/24916 [==============================] - 1s 23us/step - loss: 0.3885 - acc: 0.8239 - val_loss: 0.4562 - val_acc: 0.7882\n",
      "Epoch 3/6\n",
      "24916/24916 [==============================] - 1s 21us/step - loss: 0.3538 - acc: 0.8456 - val_loss: 0.4443 - val_acc: 0.7960\n",
      "Epoch 4/6\n",
      "24916/24916 [==============================] - 1s 24us/step - loss: 0.3290 - acc: 0.8575 - val_loss: 0.4748 - val_acc: 0.7871\n",
      "Epoch 5/6\n",
      "24916/24916 [==============================] - 1s 23us/step - loss: 0.3097 - acc: 0.8705 - val_loss: 0.4636 - val_acc: 0.7919\n",
      "Epoch 6/6\n",
      "24916/24916 [==============================] - 1s 22us/step - loss: 0.2969 - acc: 0.8761 - val_loss: 0.4746 - val_acc: 0.7937\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          4154400   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,218,657\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 4,154,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/6\n",
      "24916/24916 [==============================] - 10s 386us/step - loss: 0.4269 - acc: 0.8044 - val_loss: 0.3689 - val_acc: 0.8358\n",
      "Epoch 2/6\n",
      "24916/24916 [==============================] - 11s 442us/step - loss: 0.3236 - acc: 0.8630 - val_loss: 0.3441 - val_acc: 0.8491\n",
      "Epoch 3/6\n",
      "24916/24916 [==============================] - 11s 429us/step - loss: 0.2718 - acc: 0.8909 - val_loss: 0.3486 - val_acc: 0.8480\n",
      "Epoch 4/6\n",
      "24916/24916 [==============================] - 9s 378us/step - loss: 0.2319 - acc: 0.9115 - val_loss: 0.3296 - val_acc: 0.8592\n",
      "Epoch 5/6\n",
      "24916/24916 [==============================] - 9s 379us/step - loss: 0.1993 - acc: 0.9294 - val_loss: 0.3314 - val_acc: 0.8566\n",
      "Epoch 6/6\n",
      "24916/24916 [==============================] - 10s 399us/step - loss: 0.1688 - acc: 0.9448 - val_loss: 0.3341 - val_acc: 0.8582\n",
      "7787/7787 [==============================] - 1s 185us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.31890831587479024\n",
      "Test Accuracy: 0.8627199178500095\n",
      "[0.31890831587479024, 0.8627199178500095]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5084815]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = X[57]\n",
    "\n",
    "\n",
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Facebook API & create Dataframe (comment & timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n",
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n"
     ]
    }
   ],
   "source": [
    "def callFacebookApi():\n",
    "    url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "    response = requests.get(url_long)\n",
    "    json_data = json.loads(response.text)\n",
    "    dictFb = {'comment': [], 'time': []}\n",
    "\n",
    "    for i in range(len(json_data['posts']['data'])):\n",
    "        for j in range(len(json_data['posts']['data'][i]['comments']['data'])):\n",
    "            dictFb['comment'].append(json_data['posts']['data'][i]['comments']['data'][j]['message'])\n",
    "            dictFb['time'].append(json_data['posts']['data'][i]['comments']['data'][j]['created_time'])\n",
    "   \n",
    "    df = pd.DataFrame(dictFb, columns=['comment', 'time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n",
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time Sentiment  \n",
      "0   2019-12-06T18:59:43+0000   positiv  \n",
      "1   2019-12-06T19:00:01+0000   positiv  \n",
      "2   2019-12-06T16:10:22+0000   positiv  \n",
      "3   2019-12-13T14:53:33+0000   positiv  \n",
      "4   2019-12-13T15:05:59+0000   positiv  \n",
      "5   2019-12-13T14:53:47+0000   positiv  \n",
      "6   2019-12-13T15:17:49+0000   positiv  \n",
      "7   2019-12-13T15:02:42+0000   positiv  \n",
      "8   2019-12-13T14:56:13+0000   positiv  \n",
      "9   2019-12-13T15:08:13+0000   positiv  \n",
      "10  2019-12-13T14:48:34+0000   positiv  \n",
      "11  2019-12-13T14:49:51+0000   positiv  \n",
      "12  2019-12-13T14:58:56+0000   positiv  \n",
      "13  2019-12-13T15:04:16+0000   positiv  \n",
      "14  2019-12-13T15:16:19+0000   positiv  \n",
      "15  2019-12-13T15:11:28+0000   positiv  \n",
      "16  2019-12-13T14:44:38+0000   positiv  \n",
      "17  2019-12-13T14:41:45+0000   positiv  \n",
      "18  2019-12-13T14:59:21+0000   positiv  \n",
      "19  2019-12-13T15:14:26+0000   positiv  \n",
      "20  2019-12-13T15:12:34+0000   positiv  \n",
      "21  2019-12-13T14:47:17+0000   positiv  \n",
      "22  2019-12-13T15:10:40+0000   positiv  \n",
      "23  2019-12-13T15:00:41+0000   positiv  \n",
      "24  2019-12-13T15:15:12+0000   positiv  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>time</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>second comment 456 negativ</td>\n",
       "      <td>2019-12-06T18:59:43+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>third comment 789 positiv :) &lt;3</td>\n",
       "      <td>2019-12-06T19:00:01+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment 123 positiv</td>\n",
       "      <td>2019-12-06T16:10:22+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beautiful destinations for good prices!</td>\n",
       "      <td>2019-12-13T14:53:33+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enjoy the spectacular beaches, our delicious f...</td>\n",
       "      <td>2019-12-13T15:05:59+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High quality and a great crew!!!</td>\n",
       "      <td>2019-12-13T14:53:47+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I can't understand the other (bad) comments. I...</td>\n",
       "      <td>2019-12-13T15:17:49+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dreamy beaches, nice views, great restaurants....</td>\n",
       "      <td>2019-12-13T15:02:42+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Great service and crew!!!!!! Thank you for the...</td>\n",
       "      <td>2019-12-13T14:56:13+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I had a very comfortable stay on board. I slep...</td>\n",
       "      <td>2019-12-13T15:08:13+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I will never come back!!! On board it is dirty...</td>\n",
       "      <td>2019-12-13T14:48:34+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The staff seemed unpolite, not motivated!</td>\n",
       "      <td>2019-12-13T14:49:51+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Expensive and disappointing holiday.</td>\n",
       "      <td>2019-12-13T14:58:56+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>There is really nothing good I can say about t...</td>\n",
       "      <td>2019-12-13T15:04:16+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The doctor on board makes overpriced bills and...</td>\n",
       "      <td>2019-12-13T15:16:19+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>You ruined my vacation!</td>\n",
       "      <td>2019-12-13T15:11:28+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I don´t like this company.</td>\n",
       "      <td>2019-12-13T14:44:38+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I hate cruise ships!</td>\n",
       "      <td>2019-12-13T14:41:45+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>High prices for poor quality</td>\n",
       "      <td>2019-12-13T14:59:21+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The toilet flush was broken, the bed was uncom...</td>\n",
       "      <td>2019-12-13T15:14:26+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Loud air conditioning and dirty cabins are sta...</td>\n",
       "      <td>2019-12-13T15:12:34+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>No fridge or cofeemaker in room. Food is disgu...</td>\n",
       "      <td>2019-12-13T14:47:17+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>When you like cold food and cheeky staff - you...</td>\n",
       "      <td>2019-12-13T15:10:40+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Small cabins, no windows, ugly ports and desti...</td>\n",
       "      <td>2019-12-13T15:00:41+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The food made me sick. The drinks are disgusting.</td>\n",
       "      <td>2019-12-13T15:15:12+0000</td>\n",
       "      <td>positiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment  \\\n",
       "0                          second comment 456 negativ   \n",
       "1                     third comment 789 positiv :) <3   \n",
       "2                                 comment 123 positiv   \n",
       "3             Beautiful destinations for good prices!   \n",
       "4   Enjoy the spectacular beaches, our delicious f...   \n",
       "5                    High quality and a great crew!!!   \n",
       "6   I can't understand the other (bad) comments. I...   \n",
       "7   Dreamy beaches, nice views, great restaurants....   \n",
       "8   Great service and crew!!!!!! Thank you for the...   \n",
       "9   I had a very comfortable stay on board. I slep...   \n",
       "10  I will never come back!!! On board it is dirty...   \n",
       "11          The staff seemed unpolite, not motivated!   \n",
       "12               Expensive and disappointing holiday.   \n",
       "13  There is really nothing good I can say about t...   \n",
       "14  The doctor on board makes overpriced bills and...   \n",
       "15                            You ruined my vacation!   \n",
       "16                         I don´t like this company.   \n",
       "17                               I hate cruise ships!   \n",
       "18                       High prices for poor quality   \n",
       "19  The toilet flush was broken, the bed was uncom...   \n",
       "20  Loud air conditioning and dirty cabins are sta...   \n",
       "21  No fridge or cofeemaker in room. Food is disgu...   \n",
       "22  When you like cold food and cheeky staff - you...   \n",
       "23  Small cabins, no windows, ugly ports and desti...   \n",
       "24  The food made me sick. The drinks are disgusting.   \n",
       "\n",
       "                        time Sentiment  \n",
       "0   2019-12-06T18:59:43+0000   positiv  \n",
       "1   2019-12-06T19:00:01+0000   positiv  \n",
       "2   2019-12-06T16:10:22+0000   positiv  \n",
       "3   2019-12-13T14:53:33+0000   positiv  \n",
       "4   2019-12-13T15:05:59+0000   positiv  \n",
       "5   2019-12-13T14:53:47+0000   positiv  \n",
       "6   2019-12-13T15:17:49+0000   positiv  \n",
       "7   2019-12-13T15:02:42+0000   positiv  \n",
       "8   2019-12-13T14:56:13+0000   positiv  \n",
       "9   2019-12-13T15:08:13+0000   positiv  \n",
       "10  2019-12-13T14:48:34+0000   positiv  \n",
       "11  2019-12-13T14:49:51+0000   positiv  \n",
       "12  2019-12-13T14:58:56+0000   positiv  \n",
       "13  2019-12-13T15:04:16+0000   positiv  \n",
       "14  2019-12-13T15:16:19+0000   positiv  \n",
       "15  2019-12-13T15:11:28+0000   positiv  \n",
       "16  2019-12-13T14:44:38+0000   positiv  \n",
       "17  2019-12-13T14:41:45+0000   positiv  \n",
       "18  2019-12-13T14:59:21+0000   positiv  \n",
       "19  2019-12-13T15:14:26+0000   positiv  \n",
       "20  2019-12-13T15:12:34+0000   positiv  \n",
       "21  2019-12-13T14:47:17+0000   positiv  \n",
       "22  2019-12-13T15:10:40+0000   positiv  \n",
       "23  2019-12-13T15:00:41+0000   positiv  \n",
       "24  2019-12-13T15:15:12+0000   positiv  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze():\n",
    "    df_comments = callFacebookApi()\n",
    "    #print(df_comments)\n",
    "    # iterate over dataframe\n",
    "    for el in df['posts']:\n",
    "        el = tokenizer.texts_to_sequences(el)\n",
    "        flat_list = []\n",
    "        for sublist in el:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        flat_list = [flat_list]\n",
    "        el = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "        \n",
    "        predictvalue = model.predict(el)\n",
    "        \n",
    "        if (predictvalue < 0.5):\n",
    "            sentiment = 'negativ'\n",
    "            df_comments.insert(2, \"Sentiment\", sentiment, True)\n",
    "        elif (predictvalue > 0.5):\n",
    "            sentiment = 'positiv'\n",
    "            df_comments.insert(2, \"Sentiment\", sentiment, True)\n",
    "    print(df_comments)\n",
    "    return df_comments\n",
    "\n",
    "analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    df_facebook = callFacebookApi()\n",
    "    print(df_facebook.head())\n",
    "   \n",
    "    sentiment = ''\n",
    "    for spost in df_facebook['comment']:\n",
    "        print('test', test_post)\n",
    "        test_post_tokens = tokenizer.texts_to_sequences(test_post)\n",
    "        print(test_post_tokens)\n",
    "        flat_list = []\n",
    "        for sublist in instance:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "                flat_list = [flat_list]\n",
    "                test_post_tokens = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "                model.predict(test_post_tokens)\n",
    "\n",
    "                if (model.predict(test_post_tokens) < 0.5):\n",
    "                    sentiment = 'negativ'\n",
    "                    df_facebook.insert(2, \"Sentiment\", sentiment, True)\n",
    "                elif (model.predict(test_post_tokens) > 0.5):\n",
    "                    sentiment = 'positiv'\n",
    "    df_facebook.insert(2, \"Sentiment\", sentiment, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "response = requests.get(url_long)\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "#dictFb = {'comment': [], 'person': [], 'time': []}\n",
    "\n",
    "for i in range(len(json_data['posts']['data'])):\n",
    "    #print(i)\n",
    "    for j in range(len(json_data['posts']['data'][i]['comments']['data'])):\n",
    "        posts = json_data['posts']['data'][i]['comments']['data'][j]['message']\n",
    "        time = json_data['posts']['data'][i]['comments']['data'][j]['created_time']\n",
    "        print(type(posts))\n",
    "        \n",
    "#for i in posts:\n",
    " #   for j in time:\n",
    "#df = pd.DataFrame({\"posts\":posts[0], \"time\":time[0]}) \n",
    "\n",
    "        #df.append({'comment': comment, 'person':person, 'time':time}, ignore_index=True)\n",
    "        #df = pd.DataFrame(dictFb, columns=['comment', 'person', 'time'])\n",
    "        \n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in df_test['Description']:\n",
    "    print(type(post)) \n",
    "    test_post_tokens = tokenizer.texts_to_sequences(post)\n",
    "   #test_post_tokens = tokenizer.texts_to_sequences(post) \n",
    "    print(test_post_tokens)\n",
    "    flat_list = []\n",
    "    for sublist in instance:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "            flat_list = [flat_list]\n",
    "            test_post_tokens = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "            model.predict(test_post_tokens)  \n",
    "            if(model.predict(test_post_tokens)<0.5):\n",
    "                negative_counter = negative_counter + 1\n",
    "                df_test['Prediction'] = 'negative'\n",
    "                print('no of pos', positive_counter)\n",
    "            elif(model.predict(test_post_tokens>0.5)):\n",
    "                positive_counter = positive_counter + 1\n",
    "                print('no of pos', positive_counter)\n",
    "                df_test['Prediction'] = 'positive'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def readCSVandConvert():\n",
    "    df_test = pd.read_csv('./measuring-customer-happiness/test_hp.csv', usecols=[\"Description\"], encoding='utf-8')\n",
    "    df_test['Prediction'] = 'default value' \n",
    "    df_test.head()\n",
    "    return df_test\n",
    "            \n",
    "           \n",
    "\n",
    "readCSVandConvert()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    df_test = pd.read_csv('./measuring-customer-happiness/test_hp.csv', usecols=[\"Description\"], encoding='utf-8')\n",
    "    df_test['Prediction'] = 'default value'   \n",
    "    negative_counter = 0\n",
    "    positive_counter = 0\n",
    "    for test_post in df_test:\n",
    "        classify(test_post) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Facebook Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    " def callFacebookApi(self):\n",
    "        url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "\n",
    "        response = requests.get(url_long)\n",
    "        json_data = json.loads(response.text)\n",
    "        # df = pd.DataFrame(columns=['comment', 'person', 'time'])\n",
    "        dictFb = {'comment': [], 'person': [], 'time': []}\n",
    "        for i in range(len(json_data['posts']['data']) - 1):\n",
    "            dictFb['comment'].append(json_data['posts']['data'][i]['comments']['data'][i]['message'])\n",
    "            dictFb['person'].append(json_data['posts']['data'][i]['comments']['data'][i]['from']['name'])\n",
    "            dictFb['time'].append(json_data['posts']['data'][i]['comments']['data'][i]['created_time'])\n",
    "            # df.append({'comment': comment, 'person':person, 'time':time}, ignore_index=True)\n",
    "        df = pd.DataFrame(dictFb, columns=['comment', 'person', 'time'])\n",
    "        return df\n",
    "\n",
    "def test_data(self):\n",
    "    df_facebook = self.callFacebookApi()\n",
    "    df_facebook.head()\n",
    "    sentiment = ''\n",
    "    j = 0\n",
    "    for test_post in df_facebook:\n",
    "        test_post = tokenizer.texts_to_sequences(test_post)\n",
    "        flat_list = []\n",
    "        for sublist in instance:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "                flat_list = [flat_list]\n",
    "                test_post = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "                self.model.predict(test_post)\n",
    "\n",
    "                if (self.model.predict(test_post) < 0.5):\n",
    "                    sentiment = 'negativ'\n",
    "                    df_facebook.insert(2, \"Sentiment\", sentiment, True)\n",
    "                elif (self.model.predict(test_post) > 0.5):\n",
    "                    sentiment = 'positiv'\n",
    "                    df_facebook.insert(2, \"Sentiment\", sentiment, True)\n",
    "        ++j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_data() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-c0535b86d8a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: test_data() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>second comment 456 negativ</td>\n",
       "      <td>Lachmann Cruises</td>\n",
       "      <td>2019-12-06T18:59:43+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      comment            person                      time\n",
       "0  second comment 456 negativ  Lachmann Cruises  2019-12-06T18:59:43+0000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from threading import Thread, Event\n",
    "import time\n",
    "\n",
    "\n",
    "## below facebook api call\n",
    "def callFacebookApi():\n",
    "    url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "    other = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBAFjjW7u7xSZCSCC0oyUBQttBDoq2F1tiJwR10sV3wwEe0f3eH6FIOGNubNci7Ao4K3wHnM29TOYVVZCLXJz059uSWYjLaD4MhOGsHzGBB9uSYZB7fkH7pLTkbX1a2poC5GwtY5IXqMUhEZABWEdRPZCBQDXrb7ZBl5ygW3MMifix0P8ii7AeYZD\"\n",
    "    response = requests.get(other)\n",
    "    json_data = json.loads(response.text)\n",
    "    dictFb = {'comment': [], 'person':[], 'time':[]}\n",
    "    for i in range(len(json_data['posts']['data'])-1):\n",
    "        dictFb['comment'].append(json_data['posts']['data'][i]['comments']['data'][i]['message'])\n",
    "        dictFb['person'].append(json_data['posts']['data'][i]['comments']['data'][i]['from']['name'])\n",
    "        time = json_data['posts']['data'][i]['comments']['data'][i]['created_time']\n",
    "        dictFb['time'].append(json_data['posts']['data'][i]['comments']['data'][i]['created_time'])\n",
    "    df = pd.DataFrame(dictFb,columns=['comment', 'person', 'time'])\n",
    "    return df\n",
    "\n",
    "callFacebookApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '120017582730394', 'name': 'Lachmann Cruises', 'posts': {'data': [{'comments': {'data': [{'created_time': '2019-12-06T18:59:43+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'second comment 456 negativ', 'id': '160456975353121_165097008222451'}, {'created_time': '2019-12-06T19:00:01+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'third comment 789 positiv :) <3', 'id': '160456975353121_165097384889080'}, {'created_time': '2019-12-06T16:10:22+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'comment 123 positiv', 'id': '160456975353121_165045801560905'}], 'paging': {'cursors': {'before': 'MwZDZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160456975353121'}, {'comments': {'data': [{'created_time': '2019-12-13T14:53:33+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Beautiful destinations for good prices!', 'id': '160454742020011_168173377914814'}, {'created_time': '2019-12-13T15:05:59+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Enjoy the spectacular beaches, our delicious food and the great destinations', 'id': '160454742020011_168178507914301'}, {'created_time': '2019-12-13T14:53:47+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'High quality and a great crew!!!', 'id': '160454742020011_168173477914804'}, {'created_time': '2019-12-13T15:17:49+0000', 'message': \"I can't understand the other (bad) comments. I liked the food, the crew was always friendly. I had a good time... Thank you Lachmann Cruises!\", 'id': '160454742020011_168185177913634'}, {'created_time': '2019-12-13T15:02:42+0000', 'message': 'Dreamy beaches, nice views, great restaurants. I love it.', 'id': '160454742020011_168177391247746'}, {'created_time': '2019-12-13T14:56:13+0000', 'message': 'Great service and crew!!!!!! Thank you for the best holiday i´ve ever had. I will definitely come back soon.', 'id': '160454742020011_168174117914740'}, {'created_time': '2019-12-13T15:08:13+0000', 'message': 'I had a very comfortable stay on board. I slept well, I had shuttle service back to the airport which was awesome. Thank you so much.', 'id': '160454742020011_168179364580882'}, {'created_time': '2019-12-13T14:48:34+0000', 'message': 'I will never come back!!! On board it is dirty and cold.', 'id': '160454742020011_168171091248376'}, {'created_time': '2019-12-13T14:49:51+0000', 'message': 'The staff seemed unpolite, not motivated!', 'id': '160454742020011_168171511248334'}, {'created_time': '2019-12-13T14:58:56+0000', 'message': 'Expensive and disappointing holiday.', 'id': '160454742020011_168175304581288'}, {'created_time': '2019-12-13T15:04:16+0000', 'message': 'There is really nothing good I can say about this company', 'id': '160454742020011_168177871247698'}, {'created_time': '2019-12-13T15:16:19+0000', 'message': 'The doctor on board makes overpriced bills and gives an aspirin for hundreds of dollar....', 'id': '160454742020011_168183304580488'}, {'created_time': '2019-12-13T15:11:28+0000', 'message': 'You ruined my vacation!', 'id': '160454742020011_168180654580753'}, {'created_time': '2019-12-13T14:44:38+0000', 'message': 'I don´t like this company.', 'id': '160454742020011_168169247915227'}, {'created_time': '2019-12-13T14:41:45+0000', 'message': 'I hate cruise ships!', 'id': '160454742020011_168168211248664'}, {'created_time': '2019-12-13T14:59:21+0000', 'message': 'High prices for poor quality', 'id': '160454742020011_168176544581164'}, {'created_time': '2019-12-13T15:14:26+0000', 'message': 'The toilet flush was broken, the bed was uncomfortable. I should pay money for that? Ridiculous...', 'id': '160454742020011_168181617913990'}, {'created_time': '2019-12-13T15:12:34+0000', 'message': 'Loud air conditioning and dirty cabins are standard', 'id': '160454742020011_168181231247362'}, {'created_time': '2019-12-13T14:47:17+0000', 'message': 'No fridge or cofeemaker in room. Food is disgusting.', 'id': '160454742020011_168170334581785'}, {'created_time': '2019-12-13T15:10:40+0000', 'message': 'When you like cold food and cheeky staff - you should try Lachmann Cruises. Poor Quality!! Expensive and not worth to travel with this shabby vessel.', 'id': '160454742020011_168180447914107'}, {'created_time': '2019-12-13T15:00:41+0000', 'message': 'Small cabins, no windows, ugly ports and destinations', 'id': '160454742020011_168176797914472'}, {'created_time': '2019-12-13T15:15:12+0000', 'message': 'The food made me sick. The drinks are disgusting.', 'id': '160454742020011_168181767913975'}], 'paging': {'cursors': {'before': 'MjIZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160454742020011'}], 'paging': {'cursors': {'before': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk5UazFPRFEzTXpNeE1EYzFOalEzTkRnNU13OE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTJPVGMxTXpVek1USXhEd1IwYVcxbEJsM2RPQkFC', 'after': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk9ETTNPRGN5TVRBME5UYzNOak0yTURreU1nOE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTBOelF5TURJd01ERXhEd1IwYVcxbEJsM2ROc1FC'}}}}\n",
      "Hey there! I timed out! You can do things after me!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 42, in do_actions\n",
      "    callFacebookApi()\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 25, in callFacebookApi\n",
      "    datetime.datetime.strptime(df['time'], \"%Y-%m-%dT%H:%M:%S+0000\")\n",
      "UnboundLocalError: local variable 'df' referenced before assignment\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '120017582730394', 'name': 'Lachmann Cruises', 'posts': {'data': [{'comments': {'data': [{'created_time': '2019-12-06T18:59:43+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'second comment 456 negativ', 'id': '160456975353121_165097008222451'}, {'created_time': '2019-12-06T19:00:01+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'third comment 789 positiv :) <3', 'id': '160456975353121_165097384889080'}, {'created_time': '2019-12-06T16:10:22+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'comment 123 positiv', 'id': '160456975353121_165045801560905'}], 'paging': {'cursors': {'before': 'MwZDZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160456975353121'}, {'comments': {'data': [{'created_time': '2019-12-13T14:53:33+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Beautiful destinations for good prices!', 'id': '160454742020011_168173377914814'}, {'created_time': '2019-12-13T15:05:59+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Enjoy the spectacular beaches, our delicious food and the great destinations', 'id': '160454742020011_168178507914301'}, {'created_time': '2019-12-13T14:53:47+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'High quality and a great crew!!!', 'id': '160454742020011_168173477914804'}, {'created_time': '2019-12-13T15:17:49+0000', 'message': \"I can't understand the other (bad) comments. I liked the food, the crew was always friendly. I had a good time... Thank you Lachmann Cruises!\", 'id': '160454742020011_168185177913634'}, {'created_time': '2019-12-13T15:02:42+0000', 'message': 'Dreamy beaches, nice views, great restaurants. I love it.', 'id': '160454742020011_168177391247746'}, {'created_time': '2019-12-13T14:56:13+0000', 'message': 'Great service and crew!!!!!! Thank you for the best holiday i´ve ever had. I will definitely come back soon.', 'id': '160454742020011_168174117914740'}, {'created_time': '2019-12-13T15:08:13+0000', 'message': 'I had a very comfortable stay on board. I slept well, I had shuttle service back to the airport which was awesome. Thank you so much.', 'id': '160454742020011_168179364580882'}, {'created_time': '2019-12-13T14:48:34+0000', 'message': 'I will never come back!!! On board it is dirty and cold.', 'id': '160454742020011_168171091248376'}, {'created_time': '2019-12-13T14:49:51+0000', 'message': 'The staff seemed unpolite, not motivated!', 'id': '160454742020011_168171511248334'}, {'created_time': '2019-12-13T14:58:56+0000', 'message': 'Expensive and disappointing holiday.', 'id': '160454742020011_168175304581288'}, {'created_time': '2019-12-13T15:04:16+0000', 'message': 'There is really nothing good I can say about this company', 'id': '160454742020011_168177871247698'}, {'created_time': '2019-12-13T15:16:19+0000', 'message': 'The doctor on board makes overpriced bills and gives an aspirin for hundreds of dollar....', 'id': '160454742020011_168183304580488'}, {'created_time': '2019-12-13T15:11:28+0000', 'message': 'You ruined my vacation!', 'id': '160454742020011_168180654580753'}, {'created_time': '2019-12-13T14:44:38+0000', 'message': 'I don´t like this company.', 'id': '160454742020011_168169247915227'}, {'created_time': '2019-12-13T14:41:45+0000', 'message': 'I hate cruise ships!', 'id': '160454742020011_168168211248664'}, {'created_time': '2019-12-13T14:59:21+0000', 'message': 'High prices for poor quality', 'id': '160454742020011_168176544581164'}, {'created_time': '2019-12-13T15:14:26+0000', 'message': 'The toilet flush was broken, the bed was uncomfortable. I should pay money for that? Ridiculous...', 'id': '160454742020011_168181617913990'}, {'created_time': '2019-12-13T15:12:34+0000', 'message': 'Loud air conditioning and dirty cabins are standard', 'id': '160454742020011_168181231247362'}, {'created_time': '2019-12-13T14:47:17+0000', 'message': 'No fridge or cofeemaker in room. Food is disgusting.', 'id': '160454742020011_168170334581785'}, {'created_time': '2019-12-13T15:10:40+0000', 'message': 'When you like cold food and cheeky staff - you should try Lachmann Cruises. Poor Quality!! Expensive and not worth to travel with this shabby vessel.', 'id': '160454742020011_168180447914107'}, {'created_time': '2019-12-13T15:00:41+0000', 'message': 'Small cabins, no windows, ugly ports and destinations', 'id': '160454742020011_168176797914472'}, {'created_time': '2019-12-13T15:15:12+0000', 'message': 'The food made me sick. The drinks are disgusting.', 'id': '160454742020011_168181767913975'}], 'paging': {'cursors': {'before': 'MjIZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160454742020011'}], 'paging': {'cursors': {'before': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk5UazFPRFEzTXpNeE1EYzFOalEzTkRnNU13OE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTJPVGMxTXpVek1USXhEd1IwYVcxbEJsM2RPQkFC', 'after': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk9ETTNPRGN5TVRBME5UYzNOak0yTURreU1nOE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTBOelF5TURJd01ERXhEd1IwYVcxbEJsM2ROc1FC'}}}}\n",
      "Hey there! I timed out! You can do things after me!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 81, in do_actions\n",
      "    callFacebookApi()\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 25, in callFacebookApi\n",
      "    datetime.datetime.strptime(df['time'], \"%Y-%m-%dT%H:%M:%S+0000\")\n",
      "UnboundLocalError: local variable 'df' referenced before assignment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from threading import Thread, Event\n",
    "import time\n",
    " \n",
    "df = pd.DataFrame()\n",
    "\n",
    "## below facebook api call\n",
    "def callFacebookApi():\n",
    "\n",
    "    url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "\n",
    "    response = requests.get(url_long)\n",
    "    json_data = json.loads(response.text)\n",
    "    print(json_data)\n",
    "    #df = pd.DataFrame(columns=['comment', 'person', 'time'])\n",
    "    dictFb = {'comment': [], 'person':[], 'time':[]}\n",
    "    for i in range(len(json_data['posts']['data'])-1):\n",
    "        dictFb['comment'].append(json_data['posts']['data'][i]['comments']['data'][i]['message'])\n",
    "        dictFb['person'].append(json_data['posts']['data'][i]['comments']['data'][i]['from']['name'])\n",
    "        time = json_data['posts']['data'][i]['comments']['data'][i]['created_time']\n",
    "\n",
    "        datetime.datetime.strptime(df['time'], \"%Y-%m-%dT%H:%M:%S+0000\")\n",
    "        dictFb['time'].append(json_data['posts']['data'][i]['comments']['data'][i]['created_time'])\n",
    "        #df.append({'comment': comment, 'person':person, 'time':time}, ignore_index=True)\n",
    "    df = df(dictFb,columns=['comment', 'person', 'time'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Event object used to send signals from one thread to another\n",
    "stop_event = Event()\n",
    "\n",
    "def do_actions():\n",
    "    \"\"\"\n",
    "    Function that should timeout after 5 seconds. It simply prints a number and waits 1 second.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        callFacebookApi()\n",
    "        # 1 second\n",
    "        #time.sleep(1)\n",
    "        # 30 seconds\n",
    "        time.sleep(30)\n",
    "        # 1 day\n",
    "        #time.sleep(3600) # 1 hour \n",
    "        # Here we make the check if the other thread sent a signal to stop execution.\n",
    "        if stop_event.is_set():\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We create another Thread\n",
    "    action_thread = Thread(target=do_actions)\n",
    "\n",
    "    # Here we start the thread and we wait 5 seconds before the code continues to execute.\n",
    "    action_thread.start()\n",
    "    action_thread.join(timeout=90) # 1,5 minutes\n",
    "    #action_thread.join(timeout=86400) # 1 day\n",
    "\n",
    "    # We send a signal that the other thread should stop.\n",
    "    stop_event.set()\n",
    "\n",
    "    print(\"Hey there! I timed out! You can do things after me!\") \n",
    "    \n",
    "from threading import Thread, Event\n",
    "import time\n",
    " \n",
    "# Event object used to send signals from one thread to another\n",
    "stop_event = Event()\n",
    "\n",
    "def do_actions():\n",
    "    \"\"\"\n",
    "    Function that should timeout after 5 seconds. It simply prints a number and waits 1 second.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        callFacebookApi()\n",
    "        # 1 second\n",
    "        #time.sleep(1)\n",
    "        # 30 seconds\n",
    "        time.sleep(30)\n",
    "        # 1 day\n",
    "        #time.sleep(3600) # 1 hour \n",
    "        # Here we make the check if the other thread sent a signal to stop execution.\n",
    "        if stop_event.is_set():\n",
    "            break\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # We create another Thread\n",
    "    action_thread = Thread(target=do_actions)\n",
    " \n",
    "    # Here we start the thread and we wait 5 seconds before the code continues to execute.\n",
    "    action_thread.start()\n",
    "    action_thread.join(timeout=90) # 1,5 minutes\n",
    "    #action_thread.join(timeout=86400) # 1 day\n",
    " \n",
    "    # We send a signal that the other thread should stop.\n",
    "    stop_event.set()\n",
    " \n",
    "    print(\"Hey there! I timed out! You can do things after me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb =FacebookApi()\n",
    "fb.callFacebookApi()\n",
    "print(fb.callFacebookApi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDay(date): \n",
    "    born = datetime.datetime.strptime(date, '%d %m %Y').weekday() \n",
    "    return (calendar.day_name[born]) \n",
    "  \n",
    "# Driver program \n",
    "date = '03 02 2019'\n",
    "print(findDay(date)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load facebook comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = X[57]\n",
    "\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  - convert review into numeric form (using the tokenizer)\n",
    "####  - text_to_sequences method will convert the sentence into its numeric counter part\n",
    "####  - positive = 1, negative = 0\n",
    "####  - sigmoid function predicts floating value between 0 and 1. \n",
    "####  - value < 0.5 = negative sentiment \n",
    "####  - value > 0.5 = positive sentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "\n",
    "port = 587  # For starttls\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "receiver_email = \"burghard.lachmann@gmail.com\"\n",
    "sender_email = \"HH.Analytica@gmail.com\"\n",
    "password = 'WduenPa!19'\n",
    "message = \"\"\"\n",
    "Report von HH Analytica\n",
    "\"\"\"\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP(smtp_server, port) as server:\n",
    "   # server.ehlo()  # Can be omitted\n",
    "    server.starttls(context=context)\n",
    "    #server.ehlo()  # Can be omitted\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtplib import SMTP\n",
    "with SMTP(\"https://www.web.de\") as smtp:\n",
    "    smtp.noop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = X[57]\n",
    "\n",
    "print(instance)\n",
    "\n",
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "print(instance)\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
