{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID                                        Description  \\\n",
      "0  id10326  The room was kind of clean but had a VERY stro...   \n",
      "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
      "2  id10328  I booked this hotel through Hotwire at the low...   \n",
      "\n",
      "        Browser_Used Device_Used Is_Response  \n",
      "0               Edge      Mobile   not happy  \n",
      "1  Internet Explorer      Mobile   not happy  \n",
      "2            Mozilla      Tablet   not happy  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#df = pd.Dataframe()\n",
    "df_train = pd.read_csv('./measuring-customer-happiness/train_hp.csv', encoding='utf-8')\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "X = []\n",
    "sentences = list(df_train['Description'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stayed here with husband and sons on the way to an Alaska Cruise We all loved the hotel great experience Ask for room on the North tower facing north west for the best views We had high floor with stunning view of the needle the city and even the cruise ships We ordered room service for dinner so we could enjoy the perfect views Room service dinners were delicious too You are in perfect spot to walk everywhere so enjoy the city Almost forgot Heavenly beds were heavenly too '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification for happy and not_happy \n",
    "\n",
    "y = df_train['Is_Response']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"happy\" else 0, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding layer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('./glove.twitter.27B/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          4154400   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 4,164,401\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 4,154,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/6\n",
      "24916/24916 [==============================] - 1s 40us/step - loss: 0.4898 - acc: 0.7654 - val_loss: 0.4423 - val_acc: 0.7952\n",
      "Epoch 2/6\n",
      "24916/24916 [==============================] - 1s 23us/step - loss: 0.3885 - acc: 0.8239 - val_loss: 0.4562 - val_acc: 0.7882\n",
      "Epoch 3/6\n",
      "24916/24916 [==============================] - 1s 21us/step - loss: 0.3538 - acc: 0.8456 - val_loss: 0.4443 - val_acc: 0.7960\n",
      "Epoch 4/6\n",
      "24916/24916 [==============================] - 1s 24us/step - loss: 0.3290 - acc: 0.8575 - val_loss: 0.4748 - val_acc: 0.7871\n",
      "Epoch 5/6\n",
      "24916/24916 [==============================] - 1s 23us/step - loss: 0.3097 - acc: 0.8705 - val_loss: 0.4636 - val_acc: 0.7919\n",
      "Epoch 6/6\n",
      "24916/24916 [==============================] - 1s 22us/step - loss: 0.2969 - acc: 0.8761 - val_loss: 0.4746 - val_acc: 0.7937\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          4154400   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,218,657\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 4,154,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/6\n",
      "24916/24916 [==============================] - 10s 386us/step - loss: 0.4269 - acc: 0.8044 - val_loss: 0.3689 - val_acc: 0.8358\n",
      "Epoch 2/6\n",
      "24916/24916 [==============================] - 11s 442us/step - loss: 0.3236 - acc: 0.8630 - val_loss: 0.3441 - val_acc: 0.8491\n",
      "Epoch 3/6\n",
      "24916/24916 [==============================] - 11s 429us/step - loss: 0.2718 - acc: 0.8909 - val_loss: 0.3486 - val_acc: 0.8480\n",
      "Epoch 4/6\n",
      "24916/24916 [==============================] - 9s 378us/step - loss: 0.2319 - acc: 0.9115 - val_loss: 0.3296 - val_acc: 0.8592\n",
      "Epoch 5/6\n",
      "24916/24916 [==============================] - 9s 379us/step - loss: 0.1993 - acc: 0.9294 - val_loss: 0.3314 - val_acc: 0.8566\n",
      "Epoch 6/6\n",
      "24916/24916 [==============================] - 10s 399us/step - loss: 0.1688 - acc: 0.9448 - val_loss: 0.3341 - val_acc: 0.8582\n",
      "7787/7787 [==============================] - 1s 185us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.31890831587479024\n",
      "Test Accuracy: 0.8627199178500095\n",
      "[0.31890831587479024, 0.8627199178500095]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horrible hotel staff treated me with disgusting demeaning attitude THEY WILL NOT ONLY BE REPORTED TO CHOICE and m elite diamond but WILL NEVER EVER GO NEAR THIS HOTEL AGAIN left saying my stay was just ok would rather report them to their franchise than deal with them as they clearly don care about their guests Ironically their horrible way of manging this hotel and dealing with guests surely has got to hurt their business Treating your guests with respect makes them want to return and spend more duh cannot recommend this hotel based on my experience here Their rooms are disgusting and have bed bugs \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5084815]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = X[57]\n",
    "\n",
    "print(instance)\n",
    "\n",
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Facebook API & create Dataframe (comment & timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n",
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n"
     ]
    }
   ],
   "source": [
    "def callFacebookApi():\n",
    "    url_long = \"https://graph.facebook.com/v5.0/me?fields=id%2Cname%2Cposts%7Bcomments%7D&access_token=EAAlcIv35CUUBANREEygggKozZBFTNubNFhwuDn0u3MDI1jHz4PYRGirDFxz7MoSMTz3AHu6ZCQdT0oBp0cM3a30fGzw1pfb27C6H5OGn5jxiV49TqK8yaBiZA6DKZAeR3r0yzGGFFMGZAGmf6lMDeh8elMMGFZA5z4pk8KfQ2uiDZAZB67gt1nfh\"\n",
    "    response = requests.get(url_long)\n",
    "    json_data = json.loads(response.text)\n",
    "    dictFb = {'comment': [], 'time': []}\n",
    "\n",
    "    for i in range(len(json_data['posts']['data'])):\n",
    "        for j in range(len(json_data['posts']['data'][i]['comments']['data'])):\n",
    "            dictFb['comment'].append(json_data['posts']['data'][i]['comments']['data'][j]['message'])\n",
    "            dictFb['time'].append(json_data['posts']['data'][i]['comments']['data'][j]['created_time'])\n",
    "   \n",
    "    df = pd.DataFrame(dictFb, columns=['comment', 'time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "0                          second comment 456 negativ   \n",
      "1                     third comment 789 positiv :) <3   \n",
      "2                                 comment 123 positiv   \n",
      "3             Beautiful destinations for good prices!   \n",
      "4   Enjoy the spectacular beaches, our delicious f...   \n",
      "5                    High quality and a great crew!!!   \n",
      "6   I can't understand the other (bad) comments. I...   \n",
      "7   Dreamy beaches, nice views, great restaurants....   \n",
      "8   Great service and crew!!!!!! Thank you for the...   \n",
      "9   I had a very comfortable stay on board. I slep...   \n",
      "10  I will never come back!!! On board it is dirty...   \n",
      "11          The staff seemed unpolite, not motivated!   \n",
      "12               Expensive and disappointing holiday.   \n",
      "13  There is really nothing good I can say about t...   \n",
      "14  The doctor on board makes overpriced bills and...   \n",
      "15                            You ruined my vacation!   \n",
      "16                         I don´t like this company.   \n",
      "17                               I hate cruise ships!   \n",
      "18                       High prices for poor quality   \n",
      "19  The toilet flush was broken, the bed was uncom...   \n",
      "20  Loud air conditioning and dirty cabins are sta...   \n",
      "21  No fridge or cofeemaker in room. Food is disgu...   \n",
      "22  When you like cold food and cheeky staff - you...   \n",
      "23  Small cabins, no windows, ugly ports and desti...   \n",
      "24  The food made me sick. The drinks are disgusting.   \n",
      "\n",
      "                        time  \n",
      "0   2019-12-06T18:59:43+0000  \n",
      "1   2019-12-06T19:00:01+0000  \n",
      "2   2019-12-06T16:10:22+0000  \n",
      "3   2019-12-13T14:53:33+0000  \n",
      "4   2019-12-13T15:05:59+0000  \n",
      "5   2019-12-13T14:53:47+0000  \n",
      "6   2019-12-13T15:17:49+0000  \n",
      "7   2019-12-13T15:02:42+0000  \n",
      "8   2019-12-13T14:56:13+0000  \n",
      "9   2019-12-13T15:08:13+0000  \n",
      "10  2019-12-13T14:48:34+0000  \n",
      "11  2019-12-13T14:49:51+0000  \n",
      "12  2019-12-13T14:58:56+0000  \n",
      "13  2019-12-13T15:04:16+0000  \n",
      "14  2019-12-13T15:16:19+0000  \n",
      "15  2019-12-13T15:11:28+0000  \n",
      "16  2019-12-13T14:44:38+0000  \n",
      "17  2019-12-13T14:41:45+0000  \n",
      "18  2019-12-13T14:59:21+0000  \n",
      "19  2019-12-13T15:14:26+0000  \n",
      "20  2019-12-13T15:12:34+0000  \n",
      "21  2019-12-13T14:47:17+0000  \n",
      "22  2019-12-13T15:10:40+0000  \n",
      "23  2019-12-13T15:00:41+0000  \n",
      "24  2019-12-13T15:15:12+0000  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'df_comments':                                               comment  \\\n",
       " 0                          second comment 456 negativ   \n",
       " 1                     third comment 789 positiv :) <3   \n",
       " 2                                 comment 123 positiv   \n",
       " 3             Beautiful destinations for good prices!   \n",
       " 4   Enjoy the spectacular beaches, our delicious f...   \n",
       " 5                    High quality and a great crew!!!   \n",
       " 6   I can't understand the other (bad) comments. I...   \n",
       " 7   Dreamy beaches, nice views, great restaurants....   \n",
       " 8   Great service and crew!!!!!! Thank you for the...   \n",
       " 9   I had a very comfortable stay on board. I slep...   \n",
       " 10  I will never come back!!! On board it is dirty...   \n",
       " 11          The staff seemed unpolite, not motivated!   \n",
       " 12               Expensive and disappointing holiday.   \n",
       " 13  There is really nothing good I can say about t...   \n",
       " 14  The doctor on board makes overpriced bills and...   \n",
       " 15                            You ruined my vacation!   \n",
       " 16                         I don´t like this company.   \n",
       " 17                               I hate cruise ships!   \n",
       " 18                       High prices for poor quality   \n",
       " 19  The toilet flush was broken, the bed was uncom...   \n",
       " 20  Loud air conditioning and dirty cabins are sta...   \n",
       " 21  No fridge or cofeemaker in room. Food is disgu...   \n",
       " 22  When you like cold food and cheeky staff - you...   \n",
       " 23  Small cabins, no windows, ugly ports and desti...   \n",
       " 24  The food made me sick. The drinks are disgusting.   \n",
       " \n",
       "                         time Sentiment Sentiment Sentiment Sentiment  \\\n",
       " 0   2019-12-06T18:59:43+0000   positiv   positiv   positiv   positiv   \n",
       " 1   2019-12-06T19:00:01+0000   positiv   positiv   positiv   positiv   \n",
       " 2   2019-12-06T16:10:22+0000   positiv   positiv   positiv   positiv   \n",
       " 3   2019-12-13T14:53:33+0000   positiv   positiv   positiv   positiv   \n",
       " 4   2019-12-13T15:05:59+0000   positiv   positiv   positiv   positiv   \n",
       " 5   2019-12-13T14:53:47+0000   positiv   positiv   positiv   positiv   \n",
       " 6   2019-12-13T15:17:49+0000   positiv   positiv   positiv   positiv   \n",
       " 7   2019-12-13T15:02:42+0000   positiv   positiv   positiv   positiv   \n",
       " 8   2019-12-13T14:56:13+0000   positiv   positiv   positiv   positiv   \n",
       " 9   2019-12-13T15:08:13+0000   positiv   positiv   positiv   positiv   \n",
       " 10  2019-12-13T14:48:34+0000   positiv   positiv   positiv   positiv   \n",
       " 11  2019-12-13T14:49:51+0000   positiv   positiv   positiv   positiv   \n",
       " 12  2019-12-13T14:58:56+0000   positiv   positiv   positiv   positiv   \n",
       " 13  2019-12-13T15:04:16+0000   positiv   positiv   positiv   positiv   \n",
       " 14  2019-12-13T15:16:19+0000   positiv   positiv   positiv   positiv   \n",
       " 15  2019-12-13T15:11:28+0000   positiv   positiv   positiv   positiv   \n",
       " 16  2019-12-13T14:44:38+0000   positiv   positiv   positiv   positiv   \n",
       " 17  2019-12-13T14:41:45+0000   positiv   positiv   positiv   positiv   \n",
       " 18  2019-12-13T14:59:21+0000   positiv   positiv   positiv   positiv   \n",
       " 19  2019-12-13T15:14:26+0000   positiv   positiv   positiv   positiv   \n",
       " 20  2019-12-13T15:12:34+0000   positiv   positiv   positiv   positiv   \n",
       " 21  2019-12-13T14:47:17+0000   positiv   positiv   positiv   positiv   \n",
       " 22  2019-12-13T15:10:40+0000   positiv   positiv   positiv   positiv   \n",
       " 23  2019-12-13T15:00:41+0000   positiv   positiv   positiv   positiv   \n",
       " 24  2019-12-13T15:15:12+0000   positiv   positiv   positiv   positiv   \n",
       " \n",
       "    Sentiment Sentiment Sentiment Sentiment  ... Sentiment Sentiment Sentiment  \\\n",
       " 0    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 1    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 2    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 3    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 4    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 5    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 6    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 7    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 8    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 9    positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 10   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 11   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 12   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 13   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 14   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 15   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 16   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 17   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 18   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 19   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 20   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 21   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 22   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 23   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " 24   positiv   positiv   positiv   positiv  ...   positiv   positiv   positiv   \n",
       " \n",
       "    Sentiment Sentiment Sentiment Sentiment Sentiment Sentiment Sentiment  \n",
       " 0    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 1    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 2    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 3    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 4    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 5    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 6    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 7    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 8    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 9    positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 10   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 11   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 12   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 13   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 14   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 15   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 16   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 17   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 18   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 19   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 20   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 21   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 22   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 23   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " 24   positiv   positiv   positiv   positiv   positiv   positiv   positiv  \n",
       " \n",
       " [25 rows x 27 columns],\n",
       " 'negative': 0,\n",
       " 'positive': 25,\n",
       " 'total': 25,\n",
       " 'reportnegative': False,\n",
       " 'negativepercent': 0.0,\n",
       " 'positivepercent': 100.0}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze():\n",
    "    \n",
    "    negcount = 0\n",
    "    poscount = 0\n",
    "    negseries = 0\n",
    "    \n",
    "    reportneg = False\n",
    "    \n",
    "    df_comments = callFacebookApi()\n",
    "    #print(df_comments)\n",
    "    # iterate over dataframe\n",
    "    for el in df_comments['comment']:\n",
    "        el = tokenizer.texts_to_sequences(el)\n",
    "        flat_list = []\n",
    "        for sublist in el:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        flat_list = [flat_list]\n",
    "        el = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "        \n",
    "        predictvalue = model.predict(el)\n",
    "        \n",
    "        if (predictvalue <= 0.5):\n",
    "            negcount += 1\n",
    "            negseries += 1 \n",
    "            sentiment = 'negativ'\n",
    "            df_comments.insert(2, \"Sentiment\", sentiment, True)\n",
    "            \n",
    "        elif (predictvalue > 0.5):\n",
    "            poscount += 1\n",
    "            negseries = 0\n",
    "            sentiment = 'positiv'\n",
    "            df_comments.insert(2, \"Sentiment\", sentiment, True)\n",
    "            \n",
    "        if(negseries == 5 and reportneg == False):\n",
    "            reportneg = True\n",
    "            \n",
    "            \n",
    "    return {'df_comments':df_comments,'negative': negcount, 'positive': poscount,'total': negcount+poscount, 'reportnegative':reportneg,\n",
    "           'negativepercent': negcount/(negcount+poscount)*100, 'positivepercent': poscount/(negcount+poscount)*100}\n",
    "analyze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for 5 negative comments or 40% comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '120017582730394', 'name': 'Lachmann Cruises', 'posts': {'data': [{'comments': {'data': [{'created_time': '2019-12-06T18:59:43+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'second comment 456 negativ', 'id': '160456975353121_165097008222451'}, {'created_time': '2019-12-06T19:00:01+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'third comment 789 positiv :) <3', 'id': '160456975353121_165097384889080'}, {'created_time': '2019-12-06T16:10:22+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'comment 123 positiv', 'id': '160456975353121_165045801560905'}], 'paging': {'cursors': {'before': 'MwZDZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160456975353121'}, {'comments': {'data': [{'created_time': '2019-12-13T14:53:33+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Beautiful destinations for good prices!', 'id': '160454742020011_168173377914814'}, {'created_time': '2019-12-13T15:05:59+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Enjoy the spectacular beaches, our delicious food and the great destinations', 'id': '160454742020011_168178507914301'}, {'created_time': '2019-12-13T14:53:47+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'High quality and a great crew!!!', 'id': '160454742020011_168173477914804'}, {'created_time': '2019-12-13T15:17:49+0000', 'message': \"I can't understand the other (bad) comments. I liked the food, the crew was always friendly. I had a good time... Thank you Lachmann Cruises!\", 'id': '160454742020011_168185177913634'}, {'created_time': '2019-12-13T15:02:42+0000', 'message': 'Dreamy beaches, nice views, great restaurants. I love it.', 'id': '160454742020011_168177391247746'}, {'created_time': '2019-12-13T14:56:13+0000', 'message': 'Great service and crew!!!!!! Thank you for the best holiday i´ve ever had. I will definitely come back soon.', 'id': '160454742020011_168174117914740'}, {'created_time': '2019-12-13T15:08:13+0000', 'message': 'I had a very comfortable stay on board. I slept well, I had shuttle service back to the airport which was awesome. Thank you so much.', 'id': '160454742020011_168179364580882'}, {'created_time': '2019-12-13T14:48:34+0000', 'message': 'I will never come back!!! On board it is dirty and cold.', 'id': '160454742020011_168171091248376'}, {'created_time': '2019-12-13T14:49:51+0000', 'message': 'The staff seemed unpolite, not motivated!', 'id': '160454742020011_168171511248334'}, {'created_time': '2019-12-13T14:58:56+0000', 'message': 'Expensive and disappointing holiday.', 'id': '160454742020011_168175304581288'}, {'created_time': '2019-12-13T15:04:16+0000', 'message': 'There is really nothing good I can say about this company', 'id': '160454742020011_168177871247698'}, {'created_time': '2019-12-13T15:16:19+0000', 'message': 'The doctor on board makes overpriced bills and gives an aspirin for hundreds of dollar....', 'id': '160454742020011_168183304580488'}, {'created_time': '2019-12-13T15:11:28+0000', 'message': 'You ruined my vacation!', 'id': '160454742020011_168180654580753'}, {'created_time': '2019-12-13T14:44:38+0000', 'message': 'I don´t like this company.', 'id': '160454742020011_168169247915227'}, {'created_time': '2019-12-13T14:41:45+0000', 'message': 'I hate cruise ships!', 'id': '160454742020011_168168211248664'}, {'created_time': '2019-12-13T14:59:21+0000', 'message': 'High prices for poor quality', 'id': '160454742020011_168176544581164'}, {'created_time': '2019-12-13T15:14:26+0000', 'message': 'The toilet flush was broken, the bed was uncomfortable. I should pay money for that? Ridiculous...', 'id': '160454742020011_168181617913990'}, {'created_time': '2019-12-13T15:12:34+0000', 'message': 'Loud air conditioning and dirty cabins are standard', 'id': '160454742020011_168181231247362'}, {'created_time': '2019-12-13T14:47:17+0000', 'message': 'No fridge or cofeemaker in room. Food is disgusting.', 'id': '160454742020011_168170334581785'}, {'created_time': '2019-12-13T15:10:40+0000', 'message': 'When you like cold food and cheeky staff - you should try Lachmann Cruises. Poor Quality!! Expensive and not worth to travel with this shabby vessel.', 'id': '160454742020011_168180447914107'}, {'created_time': '2019-12-13T15:00:41+0000', 'message': 'Small cabins, no windows, ugly ports and destinations', 'id': '160454742020011_168176797914472'}, {'created_time': '2019-12-13T15:15:12+0000', 'message': 'The food made me sick. The drinks are disgusting.', 'id': '160454742020011_168181767913975'}], 'paging': {'cursors': {'before': 'MjIZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160454742020011'}], 'paging': {'cursors': {'before': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk5UazFPRFEzTXpNeE1EYzFOalEzTkRnNU13OE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTJPVGMxTXpVek1USXhEd1IwYVcxbEJsM2RPQkFC', 'after': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk9ETTNPRGN5TVRBME5UYzNOak0yTURreU1nOE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTBOelF5TURJd01ERXhEd1IwYVcxbEJsM2ROc1FC'}}}}\n",
      "Hey there! I timed out! You can do things after me!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 42, in do_actions\n",
      "    callFacebookApi()\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 25, in callFacebookApi\n",
      "    datetime.datetime.strptime(df['time'], \"%Y-%m-%dT%H:%M:%S+0000\")\n",
      "UnboundLocalError: local variable 'df' referenced before assignment\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '120017582730394', 'name': 'Lachmann Cruises', 'posts': {'data': [{'comments': {'data': [{'created_time': '2019-12-06T18:59:43+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'second comment 456 negativ', 'id': '160456975353121_165097008222451'}, {'created_time': '2019-12-06T19:00:01+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'third comment 789 positiv :) <3', 'id': '160456975353121_165097384889080'}, {'created_time': '2019-12-06T16:10:22+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'comment 123 positiv', 'id': '160456975353121_165045801560905'}], 'paging': {'cursors': {'before': 'MwZDZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160456975353121'}, {'comments': {'data': [{'created_time': '2019-12-13T14:53:33+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Beautiful destinations for good prices!', 'id': '160454742020011_168173377914814'}, {'created_time': '2019-12-13T15:05:59+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'Enjoy the spectacular beaches, our delicious food and the great destinations', 'id': '160454742020011_168178507914301'}, {'created_time': '2019-12-13T14:53:47+0000', 'from': {'name': 'Lachmann Cruises', 'id': '120017582730394'}, 'message': 'High quality and a great crew!!!', 'id': '160454742020011_168173477914804'}, {'created_time': '2019-12-13T15:17:49+0000', 'message': \"I can't understand the other (bad) comments. I liked the food, the crew was always friendly. I had a good time... Thank you Lachmann Cruises!\", 'id': '160454742020011_168185177913634'}, {'created_time': '2019-12-13T15:02:42+0000', 'message': 'Dreamy beaches, nice views, great restaurants. I love it.', 'id': '160454742020011_168177391247746'}, {'created_time': '2019-12-13T14:56:13+0000', 'message': 'Great service and crew!!!!!! Thank you for the best holiday i´ve ever had. I will definitely come back soon.', 'id': '160454742020011_168174117914740'}, {'created_time': '2019-12-13T15:08:13+0000', 'message': 'I had a very comfortable stay on board. I slept well, I had shuttle service back to the airport which was awesome. Thank you so much.', 'id': '160454742020011_168179364580882'}, {'created_time': '2019-12-13T14:48:34+0000', 'message': 'I will never come back!!! On board it is dirty and cold.', 'id': '160454742020011_168171091248376'}, {'created_time': '2019-12-13T14:49:51+0000', 'message': 'The staff seemed unpolite, not motivated!', 'id': '160454742020011_168171511248334'}, {'created_time': '2019-12-13T14:58:56+0000', 'message': 'Expensive and disappointing holiday.', 'id': '160454742020011_168175304581288'}, {'created_time': '2019-12-13T15:04:16+0000', 'message': 'There is really nothing good I can say about this company', 'id': '160454742020011_168177871247698'}, {'created_time': '2019-12-13T15:16:19+0000', 'message': 'The doctor on board makes overpriced bills and gives an aspirin for hundreds of dollar....', 'id': '160454742020011_168183304580488'}, {'created_time': '2019-12-13T15:11:28+0000', 'message': 'You ruined my vacation!', 'id': '160454742020011_168180654580753'}, {'created_time': '2019-12-13T14:44:38+0000', 'message': 'I don´t like this company.', 'id': '160454742020011_168169247915227'}, {'created_time': '2019-12-13T14:41:45+0000', 'message': 'I hate cruise ships!', 'id': '160454742020011_168168211248664'}, {'created_time': '2019-12-13T14:59:21+0000', 'message': 'High prices for poor quality', 'id': '160454742020011_168176544581164'}, {'created_time': '2019-12-13T15:14:26+0000', 'message': 'The toilet flush was broken, the bed was uncomfortable. I should pay money for that? Ridiculous...', 'id': '160454742020011_168181617913990'}, {'created_time': '2019-12-13T15:12:34+0000', 'message': 'Loud air conditioning and dirty cabins are standard', 'id': '160454742020011_168181231247362'}, {'created_time': '2019-12-13T14:47:17+0000', 'message': 'No fridge or cofeemaker in room. Food is disgusting.', 'id': '160454742020011_168170334581785'}, {'created_time': '2019-12-13T15:10:40+0000', 'message': 'When you like cold food and cheeky staff - you should try Lachmann Cruises. Poor Quality!! Expensive and not worth to travel with this shabby vessel.', 'id': '160454742020011_168180447914107'}, {'created_time': '2019-12-13T15:00:41+0000', 'message': 'Small cabins, no windows, ugly ports and destinations', 'id': '160454742020011_168176797914472'}, {'created_time': '2019-12-13T15:15:12+0000', 'message': 'The food made me sick. The drinks are disgusting.', 'id': '160454742020011_168181767913975'}], 'paging': {'cursors': {'before': 'MjIZD', 'after': 'MQZDZD'}}}, 'id': '120017582730394_160454742020011'}], 'paging': {'cursors': {'before': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk5UazFPRFEzTXpNeE1EYzFOalEzTkRnNU13OE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTJPVGMxTXpVek1USXhEd1IwYVcxbEJsM2RPQkFC', 'after': 'Q2c4U1pXNTBYM0YxWlhKNVgzTjBiM0o1WDJsa0R5TXhNakF3TVRjMU9ESTNNekF6T1RRNk9ETTNPRGN5TVRBME5UYzNOak0yTURreU1nOE1ZAWEJwWDNOMGIzSjVYMmxrRHg4eE1qQXdNVGMxT0RJM016QXpPVFJmTVRZAd05EVTBOelF5TURJd01ERXhEd1IwYVcxbEJsM2ROc1FC'}}}}\n",
      "Hey there! I timed out! You can do things after me!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 81, in do_actions\n",
      "    callFacebookApi()\n",
      "  File \"<ipython-input-95-c2d3112d6aad>\", line 25, in callFacebookApi\n",
      "    datetime.datetime.strptime(df['time'], \"%Y-%m-%dT%H:%M:%S+0000\")\n",
      "UnboundLocalError: local variable 'df' referenced before assignment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from threading import Thread, Event\n",
    "import time\n",
    "\n",
    "\n",
    "# Event object used to send signals from one thread to another\n",
    "stop_event = Event()\n",
    "\n",
    "def do_actions():\n",
    "    \"\"\"\n",
    "    Function that should timeout after 5 seconds. It simply prints a number and waits 1 second.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        callFacebookApi()\n",
    "        # 1 second\n",
    "        #time.sleep(1)\n",
    "        # 30 seconds\n",
    "        time.sleep(30)\n",
    "        # 1 day\n",
    "        #time.sleep(3600) # 1 hour \n",
    "        # Here we make the check if the other thread sent a signal to stop execution.\n",
    "        if stop_event.is_set():\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We create another Thread\n",
    "    action_thread = Thread(target=do_actions)\n",
    "\n",
    "    # Here we start the thread and we wait 5 seconds before the code continues to execute.\n",
    "    action_thread.start()\n",
    "    action_thread.join(timeout=90) # 1,5 minutes\n",
    "    #action_thread.join(timeout=86400) # 1 day\n",
    "\n",
    "    # We send a signal that the other thread should stop.\n",
    "    stop_event.set()\n",
    "\n",
    "    print(\"Hey there! I timed out! You can do things after me!\") \n",
    "    \n",
    "from threading import Thread, Event\n",
    "import time\n",
    " \n",
    "# Event object used to send signals from one thread to another\n",
    "stop_event = Event()\n",
    "\n",
    "def do_actions():\n",
    "    \"\"\"\n",
    "    Function that should timeout after 5 seconds. It simply prints a number and waits 1 second.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        callFacebookApi()\n",
    "        # 1 second\n",
    "        #time.sleep(1)\n",
    "        # 30 seconds\n",
    "        time.sleep(30)\n",
    "        # 1 day\n",
    "        #time.sleep(3600) # 1 hour \n",
    "        # Here we make the check if the other thread sent a signal to stop execution.\n",
    "        if stop_event.is_set():\n",
    "            break\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # We create another Thread\n",
    "    action_thread = Thread(target=do_actions)\n",
    " \n",
    "    # Here we start the thread and we wait 5 seconds before the code continues to execute.\n",
    "    action_thread.start()\n",
    "    action_thread.join(timeout=90) # 1,5 minutes\n",
    "    #action_thread.join(timeout=86400) # 1 day\n",
    " \n",
    "    # We send a signal that the other thread should stop.\n",
    "    stop_event.set()\n",
    " \n",
    "    print(\"Hey there! I timed out! You can do things after me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PDF Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from fpdf import FPDF\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf\n",
      "  Downloading https://files.pythonhosted.org/packages/37/c6/608a9e6c172bf9124aa687ec8b9f0e8e5d697d59a5f4fad0e2d5ec2a7556/fpdf-1.7.2.tar.gz\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40722 sha256=e584ce5f992b1eaac0ea77abbc5ec3943299cdb35bb9b71ab18d09bd4ab666c0\n",
      "  Stored in directory: /Users/jf/Library/Caches/pip/wheels/9a/e9/77/4554ff5c99bc3f487c8d69620d8c41d99d54e9c54ab20ef4c9\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fpdf\n",
    "pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report:\n",
    "    fakecsv = None\n",
    "\n",
    "    def __init__(self, fakecsv):\n",
    "        self.fakecsv = fakecsv\n",
    "\n",
    "    #Variablen\n",
    "    def initialize(self):\n",
    "        count_minus1_neg = 0\n",
    "        count_minus7_neg = 0\n",
    "        count_minus31_neg = 0\n",
    "        count_minus365_neg = 0\n",
    "        count_minus1_pos = 0\n",
    "        count_minus7_pos = 0\n",
    "        count_minus31_pos = 0\n",
    "        count_minus365_pos = 0\n",
    "        now = datetime.now()\n",
    "        minus1 = datetime.today() - timedelta(days=1)\n",
    "        minus7 = datetime.today() - timedelta(days=7)\n",
    "        minus31 = datetime.today() - timedelta(days=31)\n",
    "        minus365 = datetime.today() - timedelta(days=365)\n",
    "\n",
    "        fakedata = np.loadtxt(self.fakecsv, delimiter=';')\n",
    "\n",
    "        df=pd.DataFrame()\n",
    "        df['id'] = fakedata[:, 0]\n",
    "        df['date'] = fakedata[:, 1]\n",
    "        df['month'] = fakedata[:, 2]\n",
    "        df['kw'] = fakedata[:, 3]\n",
    "        df['weekday'] = fakedata[:, 4]\n",
    "        df['day'] = fakedata[:, 7]\n",
    "        df['hour'] = fakedata[:, 8]\n",
    "        df['minute'] = fakedata[:, 9]\n",
    "        df['weekhour'] = fakedata[:, 10]\n",
    "        df['sentiment'] = fakedata[:, 11]\n",
    "        df['real_date'] = pd.TimedeltaIndex(df['date'], unit='d') + dt.datetime(1900,1,1)\n",
    "\n",
    "        array_minus1_neg = df[(df.real_date > minus1) & (df.sentiment == 2)].count()\n",
    "        array_minus7_neg = df[(df.real_date > minus7) & (df.sentiment == 2)].count()\n",
    "        array_minus31_neg = df[(df.real_date > minus31) & (df.sentiment == 2)].count()\n",
    "        array_minus365_neg = df[(df.real_date > minus365) & (df.sentiment == 2)].count()\n",
    "\n",
    "        count_minus1_neg = array_minus1_neg[0]\n",
    "        count_minus7_neg = array_minus7_neg[0]\n",
    "        count_minus31_neg = array_minus31_neg[0]\n",
    "        count_minus365_neg = array_minus365_neg[0]\n",
    "\n",
    "        array_minus1_pos = df[(df.real_date > minus1) & (df.sentiment == 1)].count()\n",
    "        array_minus7_pos = df[(df.real_date > minus7) & (df.sentiment == 1)].count()\n",
    "        array_minus31_pos = df[(df.real_date > minus31) & (df.sentiment == 1)].count()\n",
    "        array_minus365_pos = df[(df.real_date > minus365) & (df.sentiment == 1)].count()\n",
    "\n",
    "        count_minus1_pos = array_minus1_pos[0]\n",
    "        count_minus7_pos = array_minus7_pos[0]\n",
    "        count_minus31_pos = array_minus31_pos[0]\n",
    "        count_minus365_pos = array_minus365_pos[0]\n",
    "\n",
    "        sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "        # Create the data\n",
    "        rs = np.random.RandomState(1979)\n",
    "        x = rs.randn(500)\n",
    "        g = np.tile(list(\"ABCDEFGHIJ\"), 50)\n",
    "        df = pd.DataFrame(dict(x=x, g=g))\n",
    "        m = df.g.map(ord)\n",
    "        df[\"x\"] += m\n",
    "\n",
    "        # Initialize the FacetGrid object\n",
    "        pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "        g = sns.FacetGrid(dfcsv, row=\"g\", hue=\"g\", aspect=15, height=.5, palette=pal)\n",
    "\n",
    "        # Draw the densities in a few steps\n",
    "        g.map(sns.kdeplot, \"x\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\n",
    "        g.map(sns.kdeplot, \"x\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "        g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "\n",
    "        # Define and use a simple function to label the plot in axes coordinates\n",
    "        def label(x, color, label):\n",
    "            ax = plt.gca()\n",
    "            ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "                    ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "        g.map(label, \"x\")\n",
    "\n",
    "        # Set the subplots to overlap\n",
    "        g.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "        # Remove axes details that don't play well with overlap\n",
    "        g.set_titles(\"\")\n",
    "        g.set(yticks=[])\n",
    "        g.despine(bottom=True, left=True)\n",
    "\n",
    "        g.savefig(\"WeekPlot.png\")\n",
    "\n",
    "        sns.set(style=\"whitegrid\")\n",
    "\n",
    "        #rs = np.random.RandomState(365)\n",
    "        #values = rs.randn(365, 4).cumsum(axis=0)\n",
    "        #dates = pd.date_range(\"1 1 2016\", periods=365, freq=\"D\")\n",
    "        values = df[(df.real_date > minus1)].count()\n",
    "        print(values)\n",
    "        hour = df.hour\n",
    "        print(hour)\n",
    "        data = pd.DataFrame(values, hour, columns=[\"Positiv\", \"Negativ\", \"Gesant\"])\n",
    "        data = data.rolling(7).mean()\n",
    "\n",
    "        dayplot = sns.lineplot(data=data, palette=\"tab10\", linewidth=2.5)\n",
    "        print(dayplot)\n",
    "        dayplotfigure = dayplot.get_figure()\n",
    "        dayplotfigure.savefig(\"DayPlot.png\")\n",
    "\n",
    "        today = date.today()\n",
    "        strtoday = today.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "        slash = '/'\n",
    "        string_neg1 = str(count_minus1_neg)\n",
    "        string_pos1 = str(count_minus1_pos)\n",
    "        string1 = string_neg1 + slash + string_pos1\n",
    "        string_neg7 = str(count_minus7_neg)\n",
    "        string_pos7 = str(count_minus7_pos)\n",
    "        string7 = string_neg7 + slash + string_pos7\n",
    "        string_neg31 = str(count_minus31_neg)\n",
    "        string_pos31 = str(count_minus31_pos)\n",
    "        string31 = string_neg31 + slash + string_pos31\n",
    "        string_neg365 = str(count_minus365_neg)\n",
    "        string_pos365 = str(count_minus365_pos)\n",
    "        string365 = string_neg365 + slash + string_pos365\n",
    "\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_xy(0, 0)\n",
    "        pdf.image('\n",
    "                  .png', x = 0, y = 0, w = 210, h = 0, type = '', link = '')\n",
    "        pdf.set_font('arial', '', 14)\n",
    "        pdf.set_xy(108, 111)\n",
    "        pdf.image('DayPlot.png', x = None, y = None, w = 92, h = 0, type = '', link = '')\n",
    "        pdf.set_xy(19, 198)\n",
    "        pdf.image('WeekPlot.png', x = None, y = None, w = 81, h = 0, type = '', link = '')\n",
    "        pdf.set_xy(110, 198)\n",
    "        pdf.image('WeekPlot2.png', x = None, y = None, w = 81, h = 0, type = '', link = '')\n",
    "        pdf.set_xy(21, 127)\n",
    "        pdf.cell(40, 10, str(string1))\n",
    "        pdf.set_xy(67, 127)\n",
    "        pdf.cell(40, 10, str(string7))\n",
    "        pdf.set_xy(21, 161)\n",
    "        pdf.cell(40, 10, str(string31))\n",
    "        pdf.set_xy(67, 161)\n",
    "        pdf.cell(40, 10, str(string365))\n",
    "        pdf.set_xy(177, 75)\n",
    "        pdf.cell(40, 10, strtoday)\n",
    "        pdf = pdf.output('HamburgAnalytica.pdf', 'F')\n",
    "\n",
    "        return pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-e2424c071108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./FakeData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-231-c97250031d0f>\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcount_minus31_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcount_minus365_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mminus1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mminus7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "report = Report('./FakeData.csv')\n",
    "report.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "\n",
    "port = 587  # For starttls\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "receiver_email = \"burghard.lachmann@gmail.com\"\n",
    "sender_email = \"HH.Analytica@gmail.com\"\n",
    "password = 'WduenPa!19'\n",
    "message = \"\"\"\n",
    "Report von HH Analytica\n",
    "\"\"\"\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP(smtp_server, port) as server:\n",
    "   # server.ehlo()  # Can be omitted\n",
    "    server.starttls(context=context)\n",
    "    #server.ehlo()  # Can be omitted\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attach PDF Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.MIMEMultipart import MIMEMultipart\n",
    "from email.MIMEText import MIMEText\n",
    "from email.MIMEImage import MIMEImage\n",
    "msg = MIMEMultipart()\n",
    "msg.attach(MIMEText(file(\"/home/myuser/sample.pdf\").read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
